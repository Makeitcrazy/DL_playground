{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1e5b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c173d36a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c709b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec and GloVe known frameworks to execute word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e6cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {\"data\": 0, \"science\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a222f771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 0, 'science': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe31f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.Embedding(2, 5) # 2 words in vocab, 5 dimensional embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ccdf777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11efad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"data\"]], dtype=torch.long)\n",
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359eb981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0461,  0.4024, -1.0115,  0.2167, -0.6123]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Set up an embedding layer\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db05c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908f34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ed4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply word embedding on these two paragraphs and get real vectors as features\n",
    "test_sentence = \"\"\"Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI), and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organizationâ€™s data. These insights can be used to guide decision making and strategic planning.\n",
    "\"\"\"\n",
    "\n",
    "test_sentence = test_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b79cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'science', 'combines'], ['science', 'combines', 'math'], ['combines', 'math', 'and']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization to small tokens (N-grams); Unigram - single word, bigram - two words, trigram\n",
    "# Tokenize the input and build a list of tuples ([word - 2], [word - 1], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1], test_sentence[i + 2]]) for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "# print(len(test_sentence))\n",
    "\n",
    "# Print a chunk of triagrams \n",
    "print(trigrams[:3])\n",
    "\n",
    "# Put all iterable objects list tuple into a set\n",
    "vocab = set(test_sentence)\n",
    "# print(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c3b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement n-gram language modeler to extract relevant key words\n",
    "class NGramLangModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLangModeler, self).__init__()  #??\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  #??\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128) #??\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1) #?\n",
    "        return log_probs\n",
    "    \n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLangModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) # (722, 10, 2)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001) # Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c3eb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bda7dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9207ec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLangModeler(\n",
       "  (embeddings): Embedding(38, 10)\n",
       "  (linear1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31baf515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply adam optimizer\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31b1f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science combines\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'s'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\3379742019.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# 1. prepare the input to be passed to the model (i.e. turn the words into integer indices and wrap them in tensors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mcontext_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# 2. recall that torch \"accumulates\" gradients. Before passing in a new instance, zero out the gradients from the old instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\3379742019.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# 1. prepare the input to be passed to the model (i.e. turn the words into integer indices and wrap them in tensors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mcontext_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# 2. recall that torch \"accumulates\" gradients. Before passing in a new instance, zero out the gradients from the old instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 's'"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for abc, context, target in trigrams:\n",
    "        print(abc, context, target)\n",
    "        # 1. prepare the input to be passed to the model (i.e. turn the words into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype = torch.long)\n",
    "        \n",
    "        # 2. recall that torch \"accumulates\" gradients. Before passing in a new instance, zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 3. Run the forward pass, getting log probabilities over the next words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        # 4. Compute your loss function (Torch wants the target word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        \n",
    "        # 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get the python number from a 1-element tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e6c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
